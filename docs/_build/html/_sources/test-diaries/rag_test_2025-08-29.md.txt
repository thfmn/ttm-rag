# RAG Live Data Test - August 29, 2025

## üìù Objective
To rigorously test the RAG pipeline by ingesting and querying 100 new documents from a live data source, while meticulously documenting the process, performance, and outcomes to provide clear insights for project stakeholders.

---

## Phase 1: Data Acquisition

### 1.1. PubMed Query
- **Query:** `("thai traditional medicine"[Title/Abstract] OR "thai herbal medicine"[Title/Abstract]) AND ("2020"[PDAT] : "3000"[PDAT])`
- **Objective:** Fetch 100 high-quality, recent articles.

### 1.2. Acquisition Script (`scripts/acquisition/fetch_pubmed_data.py`)
*Status: Complete*

### 1.3. Execution Log
- **Timestamp:** 2025-08-29 20:33:29
- **Outcome:** Success
- **Articles Fetched:** 95
- **Output File:** `data/raw/pubmed_ttm_100_articles.json`

---

## Phase 2: Ingestion & Vectorization

### 2.1. Ingestion Script (`scripts/ingestion/ingest_documents.py`)
*Status: Complete*

### 2.2. Performance Metrics
- **Total Ingestion Time:** 16.27 seconds
- **Throughput (docs/min):** 328.25
- **Total Chunks Created:** 103
- **Average Chunks per Document:** ~1.16
- **Errors:** 0

### 2.3. Execution Log
- **Timestamp:** 2025-08-29 20:34:57
- **Outcome:** Success
- **Documents Processed:** 95
- **Successfully Ingested:** 89
- **Skipped (empty content):** 6

---

## Phase 3: Retrieval Evaluation

### 3.1. Evaluation Script (`scripts/evaluation/evaluate_retrieval.py`)
*Status: Complete*

### 3.2. Evaluation Questions & Analysis
- **Execution Timestamp:** 2025-08-29 20:36:04
- **Results File:** `results/retrieval_evaluation_results.json`
- **Summary:** The evaluation script ran successfully, but retrieval performance was poor. For 9 out of 10 questions, 0 chunks were retrieved.
- **Analysis:** The `v1/rag/statistics` endpoint shows that only 103 chunks were created from 89 documents, which is a verly low number of chunks. This suggests an issue with the chunking strategy. The default similarity threshold of `0.7` is likely too high for the small number of chunks and the `all-MiniLM-L6-v2` model. This is the most probable cause for the poor retrieval performance.

---

## Phase 4: Reporting & Summary

### 4.1. Final Report
The RAG pipeline is functional, but the retrieval quality is poor with the current configuration. The main issues identified are:
1.  **Low Chunk Count:** The number of chunks created per document is very low. This needs to be investigated and addressed.
2.  **High Similarity Threshold:** The default similarity threshold of 0.7 is likely too high for the current setup.

### 4.2. Actionable Insights
- **Recommendation 1:** Review and optimize the document chunking process in `src/rag/chunker.py` to generate more, smaller, and more meaningful chunks.
- **Recommendation 2:** Experiment with a lower similarity threshold (e.g., 0.5) to improve retrieval recall. This can be configured in the RAG pipeline.
- **Recommendation 3:** Consider using a more powerful or domain-specific embedding model if the current model is not sufficient.
